I think this is an interesting article with a noble goal: identifying general problems with evaluation metrics and how organizations can fix them. I admire the motivation behind writing the article. As somebody who has been involved at many different levels with organizations, websites, and initiatives, I have often pondered this question from many different angles. I found a lot to chew over in the article. I also ended up disagreeing with a lot of the emphasis/focus of the piece, even though I did not find much to disagree with in the individual statements.

One broad set of omissions: understating the case for metrics like pageviews. Issa has already pointed out that cost of evaluation is an important consideration. I would like to add two more key points:

- In many cases, the final outcomes are so rare and so unlikely to happen, that measurement based on those is inherently going to be flawed and highly noisy. The actual passage of immigration legislation, in a country as large as the US, is a rare (but high-impact) event. It cannot be part of an iterative development strategy.

- Many easy-to-measure metrics, such as pageviews, tend to have a wider range of possible outcomes flowing out from them than just the specific chains of causation you may come up with.

The article cautions against long chains of causation, and this caution is valid if that is the only or primary viable chain of impact. However, there are generally many potential chains of impact. To take one example, the Effective Altruism website (effective-altruism.com) may have been formed with a potential chain of causation (more people write about EA -> more people hear about EA -> more people donate to EA causes). But it may turn out that another chain of impact could be (more people write about EA -> a new, high-impact potential area is discovered -> EA becomes more efficient). It could also be (more people write about EA -> an audience develops around these writings -> somebody advertises creation of a new nonprofit -> because of the existing audience, the person finds collaborators -> the nonprofit becomes highly impactful).

Similarly, if you wrote an in-depth article on the Open Philanthropy Project's funding of cage-free campaigns, and the subsequent rollout and success of these campaigns, it's possible you'd get impact through people reading it and beginning to demand cage-free meat. Or it could be that they read it and decide to go vegan. It could also be that most laypeople don't read it at all, but animal activists read it and it helps them understand more effective approaches to advocacy.

One of the reasons for my focus on pageviews in a real-world context (i.e., not in the toy examples discussed on this blog) is that pageviews mean different things to different people, but many of them involve some sort of value being created in ways that I may not be able to predict in advance. This doesn't necessarily mean that impact is proportional to pageviews -- far from it -- but it does suggest using pageviews (not just the number, but drilling down further based on the nature of the traffic and various follow-up actions) can help discover how people are using one's content and then update one's model of how this content could be useful. Rather than coming up with an ultimate metric in advance, I think it often makes more sense to come up with an intermediate metric that, when adequately met, is highly likely to suggest at least some later-stage impact (and failure to meet that is highly likely to mean that no late-stage impact occurred), and to then drill down to understand exactly how that impact is occuring.

The same principle applies to offline efforts -- when a person gives a talk the main effect might be on the people who are convinced by the talk and change behavior as a result. The main effect might be through people who attend the talk meeting one another and forming alliances.

----

Somewhat of a sidenote:

In the rationalist and EA community, as well as in this article, I see a lot of people note the importance of preregistration, prior declaration of hypotheses, and (where available) relying on rigorous RCTs. Over time, I have come to disagree somewhat with this emphasis. I do agree that making hypotheses up as you go along tends to have various risks associated with it, but with a reasonable level of caution and some training, you can avoid most of the biggest pitfalls of doing so. In general, I think you gain much more precision by doing your own experiments, within the specific context presented to you, and gathering data associated with that, rather than relying on rigorously conducted research in a somewhat different context.

When you are doing something new -- creating an organization or website, or starting a new leafleting campaign in an existing town or campus -- it's good to be aware of past precedent where applicable. Ultimately, however, diving in and measuring as you go along, and forming and validating theories based on those measurements, is going to yield a more accurate picture.

In short, what I'd like to see more of, instead of this post, is guidance on how to "wing" it -- how to start from a loose theory, build a reasonable way of beta testing it, get some measurements, trade off between what you are measuring directly and your knowledge of the world at large, and then go from there to progressively build something more valuable. 
